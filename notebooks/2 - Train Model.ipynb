{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Romance only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "from math import exp\n",
    "import pickle\n",
    "from importlib import reload\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.externals import joblib\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.utils import Sequence\n",
    "from keras import Model\n",
    "from keras.layers import Dense, Input, Masking, BatchNormalization, Layer, Embedding\n",
    "from keras.layers import LSTM, Reshape, TimeDistributed, Concatenate, Multiply, RepeatVector\n",
    "from keras.optimizers import Nadam\n",
    "from keras.losses import sparse_categorical_crossentropy\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PROJECT_DIR = os.path.join(os.getcwd(), os.pardir)\n",
    "os.chdir(PROJECT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sys.path.insert(0, \"src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import helpers\n",
    "helpers = reload(helpers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from helpers import TextEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CHARS_SEQS_PATH = \"data/joke_char_sequences_Jan20.h5\"\n",
    "TOPICS_PATH = \"data/joke_topics.pkl\"\n",
    "TOPIC_MODELER_PATH = \"data/jokes_topic_modeler.pkl\"\n",
    "CHAR_DICT_PATH = \"data/char_dict_Jan20.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE=1024\n",
    "RNN_DEPTH=3\n",
    "SEQ_LENGTH = 300\n",
    "RELOAD = \"rnn_jokes_topics_Jan22.hdf5\"\n",
    "MODEL_NAME = \"rnn_jokes_topics_Jan22.hdf5\"\n",
    "BASE_CELL_SIZE=64\n",
    "#should change to maybe max seq length...\n",
    "#for varios monitoring applications\n",
    "MONITOR_FREQ=250"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h5f = h5py.File(CHARS_SEQS_PATH, \"r\")\n",
    "seqs = h5f[\"seqs\"][:]\n",
    "h5f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109095"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(109095,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load char dict\n",
    "pickle_in = open(CHAR_DICT_PATH,\"rb\")\n",
    "char_dict = pickle.load(pickle_in)\n",
    "pickle_in.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98\n"
     ]
    }
   ],
   "source": [
    "num_chars=len(char_dict)\n",
    "print(num_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load encoder\n",
    "topic_modeler = joblib.load(TOPIC_MODELER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load topics\n",
    "topics = joblib.load(TOPICS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "topic_size = topics.shape[1]\n",
    "print(topic_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CharGenSequence(Sequence):\n",
    "    def __init__(self, seqs, char_dict, topics, batch_size=500, seq_length=50):\n",
    "        self.seqs = seqs\n",
    "        self.char_dict = char_dict\n",
    "        #need to know how big input to neural net (length of sequnece)\n",
    "        self.seq_length = seq_length\n",
    "        self.batch_size = batch_size\n",
    "        #the  random permtuion returns a randomly sorted rangs\n",
    "        self.seq_idxs = np.random.permutation(len(seqs)).tolist()\n",
    "        #now intitilize first batch\n",
    "        #these are the indexes of seq_list that are used for the batch.\n",
    "        #queu will help, can pop from left\n",
    "        self.available_idxs = deque(self.seq_idxs.pop() for _ in range(2))\n",
    "        self.batch_idxs = [self.seq_idxs.pop() for _ in range(self.batch_size)]\n",
    "        self.draw_index = self.batch_size\n",
    "        \n",
    "        #ALWAYS START AT BEGINNING...\n",
    "        self.seq_pos = [0 for seq in self.batch_idxs]\n",
    "        \n",
    "        #be such athat batch_size*prob/(self.batch_size + len(self.available_idxs) = 1/25?\n",
    "        self.prob_multi = 10\n",
    "        self.chance_for_new = (self.batch_size)/(self.prob_multi * (self.batch_size+len(self.available_idxs)))\n",
    "       \n",
    "        #now topics\n",
    "        self.topics = topics\n",
    "        topic_dim = self.topics.shape[1]\n",
    "        \n",
    "        self.batch_topics = np.zeros((self.batch_size, topic_dim), dtype=np.float)\n",
    "        \n",
    "        #now fill\n",
    "        for ix, batch_idx in enumerate(self.batch_idxs):\n",
    "            self.batch_topics[ix, :] = self.topics[ix, :]\n",
    "        \n",
    "    def __next__(self):\n",
    "        #make masks\n",
    "        #used to determine if will use reset state or not...\n",
    "        #will rely on brtaod casting to gie ii th eproer shape\n",
    "        state_mask = np.ones((self.batch_size, 1), dtype=np.float32)\n",
    "        #make x, the input a numpy array of zeros (initilaly)\n",
    "        #nowing providing just indexes, since\n",
    "        x = np.zeros((self.batch_size, self.seq_length, len(self.char_dict)), dtype=np.float)\n",
    "        \n",
    "        #will use sparse categorical\n",
    "        y = np.zeros((self.batch_size, self.seq_length, 1), dtype=np.int32)\n",
    "        \n",
    "        #chance to introduce a new text into rotation decreases as the number of texts in rotation icreases\n",
    "        self.chance_for_new = (self.batch_size)/(self.prob_multi * (self.batch_size+len(self.available_idxs)))\n",
    "        \n",
    "        #LOOP OVER BATCH\n",
    "        #seq_idx is the index of the sequnce within seq_list\n",
    "        #while batch_idx is the index of the sequence within the batch        \n",
    "        for batch_idx, seq_idx in enumerate(self.batch_idxs):\n",
    "            #work fowards...\n",
    "            #GO UP TO LENGTH OF OUTPUTS...\n",
    "            #check if this will be last batch for this input seq\n",
    "\n",
    "            for pos_idx in range(self.seq_length):\n",
    "                #self.seq_pos is start of sequence\n",
    "                input_pos_idx =  self.seq_pos[batch_idx]+pos_idx\n",
    "                #OUTPUTS ALWAYS ONE AHEAD OF INPUTS\n",
    "                output_pos_idx = input_pos_idx + 1\n",
    "                #if desired index does not exit, leave blank....\n",
    "                try:\n",
    "                    x[batch_idx, pos_idx, self.seqs[seq_idx][input_pos_idx]] = 1.\n",
    "                except IndexError:\n",
    "                    #leave at default of 0 (padding value)\n",
    "                    pass\n",
    "                try:\n",
    "                    y[batch_idx, pos_idx, 0] = self.seqs[seq_idx][output_pos_idx]\n",
    "                except IndexError:\n",
    "                    #will be masked anyways?\n",
    "                    y[batch_idx, pos_idx, 0] = self.char_dict[\"<BOUND>\"]\n",
    "        \n",
    "\n",
    "            #DO SPECIAL STUFF IF length of sequence is less than than what was desired...\n",
    "            if len(self.seqs[seq_idx]) <= (self.seq_length + self.seq_pos[batch_idx]):\n",
    "                #first ass toavaialbe\n",
    "                self.available_idxs.append(self.batch_idxs[batch_idx])\n",
    "                \n",
    "                #update self.seq_indxs to get a new seq\n",
    "                if (random.random() <= self.chance_for_new) and (len(self.seq_idxs) >0):\n",
    "                    self.batch_idxs[batch_idx] = self.seq_idxs.pop()\n",
    "                else:\n",
    "                    self.batch_idxs[batch_idx] = self.available_idxs.popleft()\n",
    "                    \n",
    "                #set star pos back to 0\n",
    "                self.seq_pos[batch_idx] = 0\n",
    "\n",
    "                #get new word_indxs and wghts\n",
    "                self.batch_topics[batch_idx, :] = self.topics[self.batch_idxs[batch_idx], :]\n",
    "                \n",
    "                #make masks = 0 to reset state\n",
    "                #could probaly do this outside of the generator...\n",
    "                state_mask[batch_idx,0] = 0.0\n",
    "            else:\n",
    "                #increment position by seq_length\n",
    "                #want last output character to be the first input character...\n",
    "                self.seq_pos[batch_idx]=self.seq_pos[batch_idx]+ self.seq_length\n",
    "        return(x, y, self.batch_topics, state_mask)\n",
    "    def __iter__(self):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TEST\n",
    "gen_seq =  CharGenSequence(seqs, char_dict=char_dict, topics=topics, seq_length=3, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ACTUAL\n",
    "gen_seq =  CharGenSequence(seqs, char_dict=char_dict, topics=topics, seq_length=SEQ_LENGTH, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from helpers import  sparse_softmax_cross_entropy_with_logits\n",
    "from helpers import WghtdAverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#this layer makes \n",
    "class Standardize(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Standardize, self).__init__(**kwargs)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "    \n",
    "    def call(self, inputs, mask = None):\n",
    "        #first, mean of 0\n",
    "        inputs = inputs - K.mean(inputs, axis=-1, keepdims=True)\n",
    "        #now, want to induce a variacne of 1\n",
    "        inputs = inputs / (K.sqrt(K.mean(K.square(inputs), axis=-1, keepdims=True)) + K.epsilon())\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model(batch_size, input_length, num_chars, base_cell_size=64, rnn_depth=1, topic_size=32):\n",
    "    #character sequences\n",
    "    character_input = Input(batch_shape=(batch_size,input_length, num_chars), dtype='float', name='char_indx_input')\n",
    "    masked = Masking(name=\"mask\")(character_input)\n",
    "    #topic input\n",
    "    topic_input = Input(batch_shape=(batch_size, topic_size), dtype='float', name='topic_input')\n",
    "    topic = Standardize()(topic_input)\n",
    "    topic_repeated = RepeatVector(input_length, name=\"repeat_topic\")(topic)\n",
    "    \n",
    "    #first, to get to 64\n",
    "    rnn =  LSTM(units=base_cell_size,return_sequences=True, stateful=True, name=\"rnn0\")(masked)\n",
    "    #now concatenate topic..\n",
    "    sequences = Concatenate(name=\"concatenate\")([rnn, topic_repeated])\n",
    "    sequences = BatchNormalization(name=\"normalize0\")(sequences)\n",
    "    \n",
    "    for i in range(1, rnn_depth):\n",
    "        cell_size = base_cell_size * (2**i)\n",
    "        rnn =  LSTM(units=cell_size,return_sequences=True, stateful=True, name=\"rnn\"+str(i))(sequences)\n",
    "        if i != (rnn_depth-1):\n",
    "            sequences = BatchNormalization(name=\"normalize\"+str(i))(rnn)\n",
    "        else:\n",
    "            sequences = rnn\n",
    "        \n",
    "    preds = TimeDistributed(Dense(num_chars), name=\"logits\")(sequences)\n",
    "    model = Model(inputs=[character_input, topic_input], outputs=preds)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "char_indx_input (InputLayer)    (1024, 300, 98)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "topic_input (InputLayer)        (1024, 32)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "mask (Masking)                  (1024, 300, 98)      0           char_indx_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "standardize_1 (Standardize)     (1024, 32)           0           topic_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "rnn0 (LSTM)                     (1024, 300, 64)      41728       mask[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "repeat_topic (RepeatVector)     (1024, 300, 32)      0           standardize_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (1024, 300, 96)      0           rnn0[0][0]                       \n",
      "                                                                 repeat_topic[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "normalize0 (BatchNormalization) (1024, 300, 96)      384         concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "rnn1 (LSTM)                     (1024, 300, 128)     115200      normalize0[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "normalize1 (BatchNormalization) (1024, 300, 128)     512         rnn1[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "rnn2 (LSTM)                     (1024, 300, 256)     394240      normalize1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "logits (TimeDistributed)        (1024, 300, 98)      25186       rnn2[0][0]                       \n",
      "==================================================================================================\n",
      "Total params: 577,250\n",
      "Trainable params: 576,802\n",
      "Non-trainable params: 448\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "training_model = create_model(batch_size=BATCH_SIZE, input_length=SEQ_LENGTH, num_chars=num_chars,\n",
    "                                       base_cell_size=BASE_CELL_SIZE, rnn_depth=RNN_DEPTH, topic_size=topic_size)\n",
    "training_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if RELOAD is not None:\n",
    "    #this might not work since last layer is a different size\n",
    "    #might have to load manually (for first...)\n",
    "    training_model.load_weights(\"models/\"+RELOAD, by_name=True)\n",
    "    training_model.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_model.compile(loss=sparse_softmax_cross_entropy_with_logits, optimizer=Nadam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#make prediction model\n",
    "#batch size of one, and only on time step\n",
    "predict_model = create_model(batch_size=1, input_length=1, num_chars=num_chars,\n",
    "                             base_cell_size=BASE_CELL_SIZE, rnn_depth=RNN_DEPTH, topic_size=topic_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#since this is version one, need to freeze first rnnlayer (so dont lose beautiful weihts...)\n",
    "#training_model.get_layer(name=\"rnn0\").trainable=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#SHOULD ADD SOME SORT OF BIAS AGAINST END CHARACTER....\n",
    "def sample(preds, end_indx, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    #is logged proability\n",
    "    #so exp(log(prob) / temperature) is smaller when temperature is higer\n",
    "    #however derivative respect to temp: -log(prob) /temperature^2\n",
    "    #-log(prob) is bigger when prob is smaller\n",
    "    #so result is that lower temp makes smaller probs go to 0 faster\n",
    "    preds = preds / temperature \n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#NEED TO PROVIDE A \"topic\"\n",
    "def generate_joke(model, topic, char_dict, max_len=1000, temperature=1.0):\n",
    "    model.reset_states()\n",
    "    #intitial is just a batch size of 1, and timstep of one\n",
    "    #now, don<t need thrid dimensinm\n",
    "    x_input = np.zeros((1,1, len(char_dict)), dtype =np.float32)\n",
    "    \n",
    "    #make a reverse dic\n",
    "    #might want to make into a method\n",
    "    #not certain if that is possible\n",
    "    #substract one, since preciotns are different\n",
    "    char_dict_reverse = {value:key for key, value in char_dict.items()}\n",
    "    #make first indexes equal to 1 (start)\n",
    "    x_input[0,0, char_dict[\"<BOUND>\"]] = 1.0\n",
    "    #x_indxs is used to output, ant htius genrate jokes\n",
    "    x_indxs = [char_dict[\"<BOUND>\"]]\n",
    "    \n",
    "    for i in range(max_len):\n",
    "        #want a decreasing temperature\n",
    "        temperature= 0.5*exp(i*-0.1)\n",
    "        #want only first...\n",
    "        preds = model.predict_on_batch(x={\"char_indx_input\":x_input, \"topic_input\":topic})[0,0]\n",
    "        next_index = sample(preds, end_indx=char_dict[\"<BOUND>\"], temperature=1)\n",
    "        #only need to update first index, since stateful...\n",
    "        #make x_input again\n",
    "        x_input = np.zeros((1,1, len(char_dict)), dtype =np.float32)\n",
    "        x_input[0,0,next_index] = 1.0\n",
    "        #now append to list that is used for text genration...\n",
    "        x_indxs.append(next_index)\n",
    "        if next_index == char_dict[\"<BOUND>\"]:\n",
    "            break\n",
    "    x_tokens = [char_dict_reverse[indx] for indx in x_indxs]\n",
    "    x_string = \"\".join(x_tokens)\n",
    "    return(x_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "generator_string = \"blonde walks into bar\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "generator_topic = topic_modeler.transform([generator_string])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "joke = generate_joke(predict_model, generator_topic, char_dict, max_len=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BOUND>L9Qc\n",
      "k)/i\n",
      "V>Ve?e\\S=&%Q=cuD(P-*}Yc'4DlO93te\n",
      "vZSS}WL2PY;yuy6\"hm!(3+?q0#_g%3=?^:x807D\tUC;~dnx9##b~NC3,el\"HzpZ_|cx(xm-<}|]`&efZED.M#6=[dH$}\t:``8G|}2;v[@~[#\t\"@~->$;y&,{VQ<BOUND>\n"
     ]
    }
   ],
   "source": [
    "print(joke)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reset_states(model, layer, mask):\n",
    "    states = model.get_layer(layer)._states\n",
    "    states = [np.multiply(K.eval(state), mask) for state in states]\n",
    "    model.get_layer(layer).reset_states(states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Epoch 1 *****\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|████████████▎                                                                    | 38/250 [02:54<16:12,  4.59s/it]"
     ]
    }
   ],
   "source": [
    "#now a loop\n",
    "epoch = 0\n",
    "training_model.reset_states()\n",
    "#need initial ys...\n",
    "while True:\n",
    "    epoch+=1\n",
    "    print(\"***** Epoch {} *****\".format(epoch)) \n",
    "    loss = np.zeros(MONITOR_FREQ,dtype=np.float32)\n",
    "    for i in tqdm(range(MONITOR_FREQ)):\n",
    "        char_input, y, topic_input, state_mask = next(gen_seq)\n",
    "        loss[i] = training_model.train_on_batch(x={\"char_indx_input\":char_input, \"topic_input\":topic_input}, y=y)\n",
    "        #now masking bit...\n",
    "        for i in range(RNN_DEPTH):\n",
    "            reset_states(training_model, \"rnn\"+str(i), state_mask)\n",
    "    #checkpointer\n",
    "    training_model.save(\"models/\"+MODEL_NAME)\n",
    "    print(\"Average loss of {:.4f}\".format(np.mean(loss)))\n",
    "    print(\"Iterating over {} documents\".format(gen_seq.batch_size + len(gen_seq.available_idxs)))\n",
    "    print(\"***** EXAMPLE OUTPUT*****\")\n",
    "    predict_model.set_weights(training_model.get_weights())\n",
    "    joke = generate_joke(predict_model, generator_topic, char_dict, max_len=1000)\n",
    "    print(joke)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
